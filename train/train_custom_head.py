# This file is built for fast training for comparison between different
# classification/segmentation heads, with worse performance compared to
# their optimal configurations.
#
# NOTABLY, THIS FILE IS NOT DESIGNED FOR ACCURATE TRAINING WITH A CUSTOM
# HEAD! Please use train_baseline.py for that purpose.
import torch
import torch.distributed as dist
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data import Dataset, DataLoader
from torchvision.models.resnet import ResNet
import torchvision.transforms as T

import argparse
import copy
import logging
import os
from tqdm import tqdm

import utils
from train.train_baseline import (
    HeadWoLinear, load_model, train
)

class ExtractedFeatureDataset(Dataset):
    def __init__(self, features_path, use_percent=1):
        assert 0 < use_percent <= 1
        self.features_path = features_path
        file_path = os.path.join(features_path, "len.pt")
        self.num_samples = int(torch.load(file_path) * use_percent)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, index):
        file_path = os.path.join(self.features_path, f"{index}.pt")
        return torch.load(file_path)

def load_extracted_features(
    dataset_name: str,
    pretrained_model: ResNet,
    train: bool = True,
    use_percent: float = 1,
    path_to_store: str = "./data/",
) -> Dataset:
    """
    Load the extracted features generated by the pretrained model, and store
    them on disk if they do not previously exist.
    Does not utilize multiprocessing in order to avoid data races.
    """
    features_path = os.path.join(
        path_to_store,
        "extracted_features",
        dataset_name,
        "train" if train else "eval",
    )

    if utils.is_main_process():
        logger = logging.getLogger("train_custom_head")
        logger.info(f"Attempting to load pretrained features for dataset {dataset_name}...")

        device = torch.cuda.current_device()
        # If the extracted features do not exist on disk, create them on disk
        if not os.path.exists(features_path):
            logger.info("Pretrained features do not exist; creating them on disk...")
            os.makedirs(features_path)

            # Use CenterCrop (not RandomCrop) and no additional augmentations, for we
            # plan to use the exact same data throughout multiple epochs
            mean, std = utils.dataset_mean_std(dataset_name)
            dataset = utils.load_dataset(
                dataset_name, train, path_to_store,
                transform=T.Compose([
                    T.ToTensor(),
                    T.Resize(256),
                    T.CenterCrop(224),
                    T.Normalize(mean, std),
                ])
            )
            dataloader = DataLoader(dataset, batch_size=128, shuffle=False)

            # Deep copy model so that cutting off its head (to speed up inference) 
            # would not affect original model
            pretrained_model = copy.deepcopy(pretrained_model)
            pretrained_model.avgpool = nn.Identity()
            pretrained_model.fc = nn.Identity()
            pretrained_model = pretrained_model.to(device)

            # Add forwarding hook to retrieve output before classification/segmentation
            # head
            # Note: Cannot use final output (even if the two final layers have been changed
            # to identity) since that output has been flattened
            avgpool_input = []
            pretrained_model.avgpool.register_forward_pre_hook(
                lambda _, input: avgpool_input.append(input[0].detach())
            )

            # Evaluate extracted features and store them to disk
            pretrained_model.eval()
            with torch.no_grad():
                current_index = 0
                for data, labels in tqdm(dataloader):
                    pretrained_model(data.to(device))
                    # Move features to CPU before saving to disk
                    extracted_features = avgpool_input.pop().detach().cpu()
                    for i in range(len(extracted_features)):
                        file_path = os.path.join(features_path, f"{current_index}.pt")
                        # Cloning since saving a slice would save the entire tensor
                        # See https://github.com/pytorch/pytorch/issues/40157
                        feature_label_pair = (extracted_features[i].clone(), labels[i].item())
                        torch.save(feature_label_pair, file_path)
                        current_index += 1

                # Save the total number of samples on disk
                file_path = os.path.join(features_path, "len.pt")
                torch.save(current_index, file_path)

    if utils.get_world_size() > 1:
        # Ensure all processes are synchronized (to avoid other processes accessing data
        # that do not yet exist on disk)
        dist.barrier()

    return ExtractedFeatureDataset(features_path, use_percent)

class Head(nn.Module):
    def __init__(self, num_classes: int, head_wo_linear: HeadWoLinear):
        super().__init__()
        self.head = nn.Sequential(
            head_wo_linear,
            nn.Flatten(start_dim=1),
            nn.Linear(head_wo_linear.output_features, num_classes)
        )

    def forward(self, data):
        return self.head(data)

    @staticmethod
    def parse_head(num_classes: int, head_repr: str) -> nn.Module:
        """
        Build a head from a string classification/segmentation head representation.
        This representation should not include the final flatten and linear layers.
        The layers should be separated by semicolons (;), and each layer should be
        described by its LayerType followed by the required parameter(s) for this
        layer (see train_baseline.py); the LayerType and the parameters should be
        separated by a colon (:).

        An example head with an AdaptiveMaxPool2d layer with output dimensions 2x2,
        a Tanh layer, and an AdaptiveAvgPool2d layer with output dimensions 1x1,
        prior to the final flatten and linear layers would be represented as
            
            `MaxPool: 2, 2; Tanh: ; AvgPool: 1, 1`
        """
        head_wo_linear = HeadWoLinear.parse_head_wo_linear(head_repr)
        return Head(num_classes, head_wo_linear)

def main(args):
    """
    Train and evaluate custom head using pretrained features. To see how to declare
    a custom head, see the docstring for `Head.parse_head`.
    """
    device = torch.cuda.current_device()
    device = torch.device("cuda", device)
    # Make sure INFO-level logging is not ignored
    logging.basicConfig(level=logging.INFO)

    num_classes = utils.dataset_num_classes(args.dataset)
    head = Head.parse_head(num_classes, args.head).to(device)
    if utils.get_world_size() > 1:
        head = DistributedDataParallel(
            head,
            device_ids=[device],
            output_device=device
        )

    pretrained_model = load_model(
        num_classes,
        head_layers="AvgPool: 1, 1",
        from_checkpoint=args.pretrained_model_path,
        multi_gpu=False
    )

    train_dataset = load_extracted_features(
        args.dataset,
        pretrained_model,
        train=True,
        path_to_store=args.path_to_data
    )
    eval_dataset = load_extracted_features(
        args.dataset,
        pretrained_model,
        train=False,
        path_to_store=args.path_to_data
    )
    train(
        head,
        train_dataset,
        eval_dataset,
        nn.CrossEntropyLoss(),
        lr=args.learning_rate,
        batch_size=args.batch_size,
        epochs=args.epochs,
        logdir=args.logdir,
        do_logging=not args.no_logging,
    )

    if utils.is_main_process() and args.save_head:
        head_path = os.path.join(args.logdir, "final_head.pth")
        logger = logging.getLogger("train_custom_head")
        logger.info(f"Saving head parameters to {head_path}...")

        # Retrieve underlying model in case model is DDP
        if utils.get_world_size() > 1:
            head = head.module

        # Move head to CPU to ensure it can be restored on any system
        head = head.cpu()
        torch.save(head.state_dict(), head_path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--dataset", default="cifar-10", type=str.lower)
    parser.add_argument("--num-gpus", default=1, type=int)
    parser.add_argument("--pretrained-model-path", "-ptm",
                        default="./output/cifar-10-baseline.pth",
                        type=str)
    parser.add_argument("--head", default="AvgPool: 1, 1", type=str)

    parser.add_argument("--learning-rate", "-lr", default=4e-3, type=float)
    parser.add_argument("--batch-size", "-b", default=128, type=int)
    parser.add_argument("--epochs", "-ep", default=30, type=int)
    parser.add_argument("--no-logging", "-nolog", action="store_true")

    parser.add_argument("--path-to-data", default="./data/", type=str)
    parser.add_argument("--save-head", "-sh", action="store_true")
    parser.add_argument("--logdir", default="./output/", type=str)

    args = parser.parse_args()
    utils.launch(main, args.num_gpus, (args, ))
